{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f82dc11",
   "metadata": {},
   "source": [
    "# 📊 Analyse de Corrélation Dynamique des Cryptomonnaies\n",
    "\n",
    "## 🎯 Objectif du Projet\n",
    "\n",
    "Ce notebook présente une **analyse complète des corrélations dynamiques** entre cryptomonnaies majeures pour :\n",
    "- 📈 **Identifier des groupes** d'actifs avec comportements similaires\n",
    "- 🔄 **Analyser l'évolution temporelle** des corrélations sur 2 ans\n",
    "- 🎪 **Détecter les changements de régime** dans les marchés crypto\n",
    "- 🎯 **Fournir des insights** pour l'optimisation de portefeuille\n",
    "\n",
    "## 📋 Structure du Notebook\n",
    "1. Configuration et imports\n",
    "2. Collecte des données\n",
    "3. Prétraitement des données\n",
    "4. Analyse de corrélation dynamique\n",
    "5. Clustering hiérarchique\n",
    "6. Visualisation en réseau des corrélations\n",
    "7. Clustering temporel avec K-Means\n",
    "8. Détection automatique des changements de régime\n",
    "9. Clustering spectral optimisé\n",
    "10. Rapport final et conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178882e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# 📦 1. CONFIGURATION ET IMPORTS\n",
    "# ================================================================================================\n",
    "\n",
    "# Suppression des warnings pour une sortie plus propre\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === IMPORTS PRINCIPAUX ===\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === VISUALISATION ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === DONNÉES FINANCIÈRES ===\n",
    "import yfinance as yf\n",
    "\n",
    "# === MACHINE LEARNING & CLUSTERING ===\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# === ANALYSE STATISTIQUE ===\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# === ANALYSE DE RÉSEAUX ===\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "# === CONFIGURATION GRAPHIQUES ===\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (14, 8),\n",
    "    'font.size': 11,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'figure.facecolor': 'white'\n",
    "})\n",
    "\n",
    "print(\"✅ Tous les modules importés avec succès\")\n",
    "print(f\"📅 Date d'exécution : {datetime.datetime.now().strftime('%d/%m/%Y %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cc347",
   "metadata": {},
   "source": [
    "# 📦 2. COLLECTE DES DONNÉES\n",
    "\n",
    "Téléchargement des données historiques sur **2 ans** pour les 9 cryptomonnaies sélectionnées:\n",
    "\n",
    "| Crypto | Symbole | Description |\n",
    "|--------|---------|-------------|\n",
    "| **Ethereum** | ETH-USD | Plateforme smart contracts |\n",
    "| **Binance Coin** | BNB-USD | Token exchange Binance |\n",
    "| **Ripple** | XRP-USD | Réseau de paiements |\n",
    "| **Solana** | SOL-USD | Blockchain haute performance |\n",
    "| **Cardano** | ADA-USD | Blockchain académique |\n",
    "| **Polkadot** | DOT-USD | Interopérabilité blockchain |\n",
    "| **Shiba Inu** | SHIB-USD | Meme coin populaire |\n",
    "| **Litecoin** | LTC-USD | \"L'argent numérique\" |\n",
    "| **Avalanche** | AVAX-USD | Plateforme DeFi |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d06457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_crypto_data(symbols, num_days=730):\n",
    "    \"\"\"\n",
    "    Collecte les données historiques des cryptomonnaies\n",
    "    \n",
    "    Args:\n",
    "        symbols: Liste des symboles de cryptomonnaies\n",
    "        num_days: Nombre de jours d'historique (défaut: 730 = 2 ans)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec les prix de clôture quotidiens\n",
    "    \"\"\"\n",
    "    print(\"📈 Collecte des données en cours...\")\n",
    "    \n",
    "    # Configuration des dates\n",
    "    start = datetime.date.today() - datetime.timedelta(days=num_days)\n",
    "    end = datetime.date.today()\n",
    "    \n",
    "    print(f\"📅 Période : {start} → {end} ({num_days} jours)\")\n",
    "    \n",
    "    # Téléchargement des données\n",
    "    data_close = pd.DataFrame()\n",
    "    success_count = 0\n",
    "    \n",
    "    for i, symbol in enumerate(symbols, 1):\n",
    "        try:\n",
    "            print(f\"   [{i:2d}/{len(symbols)}] {symbol:<10} ... \", end=\"\")\n",
    "            data = yf.download(symbol, start=start, end=end, interval=\"1d\", progress=False)\n",
    "            \n",
    "            if not data.empty:\n",
    "                data_close[symbol] = data[\"Close\"]\n",
    "                success_count += 1\n",
    "                print(\"✅\")\n",
    "            else:\n",
    "                print(\"❌ (Données vides)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ ({str(e)[:30]}...)\")\n",
    "    \n",
    "    # Nettoyage des données\n",
    "    initial_length = len(data_close)\n",
    "    data_close.dropna(inplace=True)\n",
    "    final_length = len(data_close)\n",
    "    \n",
    "    print(f\"\\n📊 Résumé :\")\n",
    "    print(f\"   • Cryptos collectées : {success_count}/{len(symbols)}\")\n",
    "    print(f\"   • Données brutes : {initial_length:,} jours\")\n",
    "    print(f\"   • Données nettoyées : {final_length:,} jours\")\n",
    "    print(f\"   • Données supprimées : {initial_length - final_length:,} jours\")\n",
    "    \n",
    "    return data_close\n",
    "\n",
    "# === CONFIGURATION DES CRYPTOMONNAIES ===\n",
    "crypto_symbols = [\n",
    "    \"ETH-USD\",   # Ethereum\n",
    "    \"BNB-USD\",   # Binance Coin\n",
    "    \"XRP-USD\",   # Ripple\n",
    "    \"SOL-USD\",   # Solana\n",
    "    \"ADA-USD\",   # Cardano\n",
    "    \"DOT-USD\",   # Polkadot\n",
    "    \"SHIB-USD\",  # Shiba Inu\n",
    "    \"LTC-USD\",   # Litecoin\n",
    "    \"AVAX-USD\"   # Avalanche\n",
    "]\n",
    "\n",
    "# === COLLECTE DES DONNÉES ===\n",
    "data_close = collect_crypto_data(crypto_symbols, num_days=730)\n",
    "\n",
    "# Aperçu des données\n",
    "print(f\"\\n📋 Aperçu des données collectées :\")\n",
    "print(data_close.tail())\n",
    "print(f\"\\n📏 Dimensions finales : {data_close.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a57b87",
   "metadata": {},
   "source": [
    "# 🧮 3. PRÉTRAITEMENT DES DONNÉES\n",
    "\n",
    "Calcul des rendements logarithmiques pour normaliser les données et faciliter l'analyse statistique.\n",
    "Permet également de stabiliser la variance et de réduire l'asymétrie des distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_returns(price_data):\n",
    "    \"\"\"\n",
    "    Calcule les rendements logarithmiques à partir des prix\n",
    "    \n",
    "    Args:\n",
    "        price_data: DataFrame avec les prix de clôture\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec les rendements logarithmiques\n",
    "    \"\"\"\n",
    "    print(\"🔢 Calcul des rendements logarithmiques...\")\n",
    "    \n",
    "    # Calcul des rendements logarithmiques\n",
    "    log_returns = np.log(price_data / price_data.shift(1))\n",
    "    log_returns.dropna(inplace=True)\n",
    "    \n",
    "    print(f\"Rendements calculés pour {len(log_returns)} jours\")\n",
    "    \n",
    "    # Statistiques descriptives\n",
    "    print(\"\\n📈 Statistiques des rendements:\")\n",
    "    print(f\"Rendement moyen : {log_returns.mean().mean():.4f}\")\n",
    "    print(f\"Volatilité moyenne : {log_returns.std().mean():.4f}\")\n",
    "    \n",
    "    return log_returns\n",
    "\n",
    "# Calcul des rendements\n",
    "log_returns = calculate_log_returns(data_close)\n",
    "\n",
    "# Visualisation des rendements\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Graphique des prix\n",
    "data_close.plot(ax=axes[0], title=\"Évolution des Prix des Cryptomonnaies\", \n",
    "                legend=True, alpha=0.8)\n",
    "axes[0].set_ylabel(\"Prix (USD)\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique des rendements\n",
    "log_returns.plot(ax=axes[1], title=\"Rendements Logarithmiques\", \n",
    "                 legend=True, alpha=0.7)\n",
    "axes[1].set_ylabel(\"Rendements\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Prétraitement des données terminé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3206c3",
   "metadata": {},
   "source": [
    "# 📊 4. ANALYSE DE CORRÉLATION DYNAMIQUE\n",
    "\n",
    "Calcul des matrices de corrélation glissantes avec une fenêtre de 30 jours pour capturer l'évolution \n",
    "temporelle des relations entre cryptomonnaies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae70929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_correlations(returns_data, window=30):\n",
    "    \"\"\"\n",
    "    Calcule les corrélations glissantes pour analyser l'évolution temporelle\n",
    "    \n",
    "    Args:\n",
    "        returns_data: DataFrame des rendements\n",
    "        window: Taille de la fenêtre glissante (jours)\n",
    "    \n",
    "    Returns:\n",
    "        Liste des matrices de corrélation, dates correspondantes\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Calcul des corrélations glissantes (fenêtre: {window} jours)...\")\n",
    "    \n",
    "    rolling_corrs = []\n",
    "    \n",
    "    # Calcul des corrélations glissantes avec barre de progression\n",
    "    for i in range(window, len(returns_data) + 1):\n",
    "        if i % 50 == 0:  # Affichage du progrès\n",
    "            progress = (i - window + 1) / (len(returns_data) - window + 1) * 100\n",
    "            print(f\"  Progression: {progress:.1f}%\")\n",
    "        \n",
    "        corr_matrix = returns_data.iloc[i-window:i].corr()\n",
    "        rolling_corrs.append(corr_matrix)\n",
    "    \n",
    "    # Dates correspondantes\n",
    "    dates = returns_data.index[window-1:]\n",
    "    \n",
    "    print(f\"✅ {len(rolling_corrs)} matrices de corrélation calculées\")\n",
    "    print(f\"Période couverte: {dates[0].date()} à {dates[-1].date()}\")\n",
    "    \n",
    "    return rolling_corrs, dates\n",
    "\n",
    "# Calcul des corrélations glissantes\n",
    "window_size = 30\n",
    "rolling_correlations, correlation_dates = calculate_rolling_correlations(log_returns, window_size)\n",
    "\n",
    "# Sauvegarde des données pour usage ultérieur\n",
    "latest_correlation = rolling_correlations[-1]\n",
    "latest_correlation.to_csv('last_correlation_matrix.csv')\n",
    "print(f\"\\n💾 Dernière matrice de corrélation sauvegardée: last_correlation_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_correlation_evolution(rolling_corrs, dates):\n",
    "    \"\"\"\n",
    "    Visualise l'évolution des corrélations dans le temps\n",
    "    \n",
    "    Args:\n",
    "        rolling_corrs: Liste des matrices de corrélation\n",
    "        dates: Dates correspondantes\n",
    "    \"\"\"\n",
    "    print(\"📊 Création des visualisations de corrélation...\")\n",
    "    \n",
    "    # Configuration de la figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "    \n",
    "    # Indices pour début, milieu, fin\n",
    "    indices = [0, len(rolling_corrs)//2, -1]\n",
    "    labels = [\"Début\", \"Milieu\", \"Fin\"]\n",
    "    \n",
    "    for i, (idx, label) in enumerate(zip(indices, labels)):\n",
    "        correlation_matrix = rolling_corrs[idx]\n",
    "        date = dates[idx] if idx != -1 else dates[idx]\n",
    "        \n",
    "        # Création de la heatmap\n",
    "        sns.heatmap(correlation_matrix, \n",
    "                   annot=True, \n",
    "                   cmap=\"RdBu_r\", \n",
    "                   center=0,\n",
    "                   vmin=-1, \n",
    "                   vmax=1,\n",
    "                   ax=axes[i],\n",
    "                   fmt='.2f',\n",
    "                   square=True,\n",
    "                   cbar_kws={'label': 'Corrélation'})\n",
    "        \n",
    "        axes[i].set_title(f\"Corrélations {label}\\n({date.date()})\", \n",
    "                         fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Rotation des labels pour meilleure lisibilité\n",
    "        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "        axes[i].set_yticklabels(axes[i].get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.suptitle(\"Évolution des Corrélations entre Cryptomonnaies\", \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Création des visualisations\n",
    "correlation_figure = visualize_correlation_evolution(rolling_correlations, correlation_dates)\n",
    "\n",
    "# Analyse comparative début vs fin\n",
    "print(\"\\n🔍 Analyse comparative des corrélations:\")\n",
    "start_corr = rolling_correlations[0]\n",
    "end_corr = rolling_correlations[-1]\n",
    "correlation_change = end_corr - start_corr\n",
    "\n",
    "print(f\"Corrélation moyenne début: {start_corr.mean().mean():.3f}\")\n",
    "print(f\"Corrélation moyenne fin: {end_corr.mean().mean():.3f}\")\n",
    "print(f\"Changement moyen: {correlation_change.mean().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc471a9a",
   "metadata": {},
   "source": [
    "# 🌳 5. CLUSTERING HIÉRARCHIQUE\n",
    "\n",
    "Application d'un clustering hiérarchique sur la matrice de corrélation la plus récente pour \n",
    "identifier des groupes de cryptomonnaies ayant un comportement similaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hierarchical_clustering(correlation_matrix, method='ward', threshold=1.5):\n",
    "    \"\"\"\n",
    "    Effectue un clustering hiérarchique sur la matrice de corrélation\n",
    "    \n",
    "    Args:\n",
    "        correlation_matrix: Matrice de corrélation\n",
    "        method: Méthode de linkage\n",
    "        threshold: Seuil pour former les clusters\n",
    "    \n",
    "    Returns:\n",
    "        Clusters, matrice de linkage, dictionnaire des clusters\n",
    "    \"\"\"\n",
    "    print(\"🌳 Clustering hiérarchique en cours...\")\n",
    "    \n",
    "    # Calcul de la matrice de distance adaptée (pour corrélations négatives)\n",
    "    distance_matrix = np.sqrt(0.5 * (1 - correlation_matrix))\n",
    "    \n",
    "    # Conversion en matrice condensée pour linkage\n",
    "    condensed_distances = squareform(distance_matrix)\n",
    "    \n",
    "    # Clustering hiérarchique\n",
    "    linkage_matrix = linkage(condensed_distances, method=method)\n",
    "    \n",
    "    # Visualisation du dendrogramme\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    dendrogram(linkage_matrix, \n",
    "               labels=correlation_matrix.columns, \n",
    "               leaf_rotation=90,\n",
    "               leaf_font_size=12)\n",
    "    plt.title(f\"Dendrogramme - Clustering Hiérarchique ({method.title()})\", \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Cryptomonnaies\", fontsize=12)\n",
    "    plt.ylabel(\"Distance\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Attribution des clusters\n",
    "    clusters = fcluster(linkage_matrix, t=threshold, criterion='distance')\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    print(f\"\\n📊 Résultats du clustering (seuil: {threshold}):\")\n",
    "    cluster_dict = {}\n",
    "    for crypto, cluster in zip(correlation_matrix.columns, clusters):\n",
    "        if cluster not in cluster_dict:\n",
    "            cluster_dict[cluster] = []\n",
    "        cluster_dict[cluster].append(crypto)\n",
    "    \n",
    "    for cluster_id, cryptos in cluster_dict.items():\n",
    "        print(f\"  Cluster {cluster_id}: {', '.join(cryptos)}\")\n",
    "    \n",
    "    return clusters, linkage_matrix, cluster_dict\n",
    "\n",
    "# Application du clustering hiérarchique\n",
    "hierarchical_clusters, linkage_matrix, cluster_groups = perform_hierarchical_clustering(\n",
    "    latest_correlation, method='ward', threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586acda",
   "metadata": {},
   "source": [
    "# 🕸️ 6. VISUALISATION EN RÉSEAU DES CORRÉLATIONS\n",
    "\n",
    "Création d'un graphe pour visualiser les corrélations fortes entre cryptomonnaies (seuil > 0.6).\n",
    "Cette représentation permet de mieux comprendre les relations entre les actifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea353738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_network(correlation_matrix, threshold=0.6, layout_seed=42):\n",
    "    \"\"\"\n",
    "    Crée un graphe des corrélations fortes entre cryptomonnaies\n",
    "    \n",
    "    Args:\n",
    "        correlation_matrix: Matrice de corrélation\n",
    "        threshold: Seuil de corrélation pour créer une arête\n",
    "        layout_seed: Graine pour la disposition du graphe\n",
    "    \n",
    "    Returns:\n",
    "        Graphe NetworkX\n",
    "    \"\"\"\n",
    "    print(f\"🕸️ Création du graphe de corrélations (seuil: {threshold})...\")\n",
    "    \n",
    "    # Création du graphe\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Ajout des arêtes pour corrélations > seuil\n",
    "    edges_added = 0\n",
    "    for i, j in combinations(correlation_matrix.columns, 2):\n",
    "        correlation_value = correlation_matrix.loc[i, j]\n",
    "        if correlation_value > threshold:\n",
    "            G.add_edge(i, j, weight=correlation_value)\n",
    "            edges_added += 1\n",
    "    \n",
    "    print(f\"  {edges_added} connexions fortes détectées\")\n",
    "    \n",
    "    if edges_added == 0:\n",
    "        print(f\"  ⚠️ Aucune corrélation > {threshold} trouvée. Réduction du seuil...\")\n",
    "        threshold = 0.4\n",
    "        for i, j in combinations(correlation_matrix.columns, 2):\n",
    "            correlation_value = correlation_matrix.loc[i, j]\n",
    "            if correlation_value > threshold:\n",
    "                G.add_edge(i, j, weight=correlation_value)\n",
    "                edges_added += 1\n",
    "        print(f\"  {edges_added} connexions détectées avec seuil réduit à {threshold}\")\n",
    "    \n",
    "    # Calcul de la disposition\n",
    "    pos = nx.spring_layout(G, seed=layout_seed, k=3, iterations=50)\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Calcul des poids des arêtes pour la visualisation\n",
    "    edge_weights = [G[u][v]['weight'] * 8 for u, v in G.edges()]\n",
    "    edge_colors = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    \n",
    "    # Dessin du graphe\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_color='lightblue', \n",
    "                          node_size=2000, \n",
    "                          alpha=0.8)\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, \n",
    "                          width=edge_weights, \n",
    "                          edge_color=edge_colors,\n",
    "                          edge_cmap=plt.cm.Reds,\n",
    "                          alpha=0.7)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, \n",
    "                           font_size=12, \n",
    "                           font_weight='bold')\n",
    "    \n",
    "    # Ajout des poids sur les arêtes\n",
    "    edge_labels = {(u, v): f\"{G[u][v]['weight']:.2f}\" for u, v in G.edges()}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=10)\n",
    "    \n",
    "    plt.title(f\"Réseau des Corrélations Fortes (> {threshold})\", \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques du réseau\n",
    "    print(f\"\\n📈 Statistiques du réseau:\")\n",
    "    print(f\"  Nœuds: {G.number_of_nodes()}\")\n",
    "    print(f\"  Arêtes: {G.number_of_edges()}\")\n",
    "    print(f\"  Densité: {nx.density(G):.3f}\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Création du graphe de corrélations\n",
    "correlation_network = create_correlation_network(latest_correlation, threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef8164",
   "metadata": {},
   "source": [
    "# ⏰ 7. CLUSTERING TEMPOREL AVEC K-MEANS\n",
    "\n",
    "Application de K-Means sur les matrices de corrélation aplaties pour identifier des régimes \n",
    "temporels distincts dans les relations entre cryptomonnaies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c782b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_temporal_kmeans_clustering(rolling_correlations, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Effectue un clustering K-Means temporel sur les matrices de corrélation\n",
    "    \n",
    "    Args:\n",
    "        rolling_correlations: Liste des matrices de corrélation\n",
    "        n_clusters: Nombre de clusters\n",
    "    \n",
    "    Returns:\n",
    "        Labels des clusters, modèle K-Means, données aplaties\n",
    "    \"\"\"\n",
    "    print(f\"⏰ Clustering temporel K-Means ({n_clusters} clusters)...\")\n",
    "    \n",
    "    # Fonction pour aplatir la matrice de corrélation (sans diagonale)\n",
    "    def flatten_correlation_matrix(correlation_matrix):\n",
    "        mask = ~np.eye(correlation_matrix.shape[0], dtype=bool)\n",
    "        return correlation_matrix.values[mask]\n",
    "    \n",
    "    # Aplatissement de toutes les matrices\n",
    "    flattened_data = np.array([\n",
    "        flatten_correlation_matrix(corr_matrix) \n",
    "        for corr_matrix in rolling_correlations\n",
    "    ])\n",
    "    \n",
    "    print(f\"  Données préparées: {flattened_data.shape}\")\n",
    "    \n",
    "    # Application du K-Means\n",
    "    kmeans_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans_model.fit_predict(flattened_data)\n",
    "    \n",
    "    # Calcul des statistiques\n",
    "    unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    print(f\"  Répartition des clusters:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        percentage = count / len(cluster_labels) * 100\n",
    "        print(f\"    Cluster {label+1}: {count} jours ({percentage:.1f}%)\")\n",
    "    \n",
    "    return cluster_labels, kmeans_model, flattened_data\n",
    "\n",
    "def visualize_temporal_clusters(dates, cluster_labels, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Visualise l'évolution temporelle des clusters\n",
    "    \n",
    "    Args:\n",
    "        dates: Dates correspondantes\n",
    "        cluster_labels: Labels des clusters\n",
    "        n_clusters: Nombre de clusters\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Couleurs pour chaque cluster\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    # Graphique principal\n",
    "    for i in range(n_clusters):\n",
    "        mask = cluster_labels == i\n",
    "        plt.scatter(dates[mask], cluster_labels[mask] + 1, \n",
    "                   c=[colors[i]], label=f'Régime {i+1}', \n",
    "                   alpha=0.7, s=30)\n",
    "    \n",
    "    plt.ylabel(\"Régime de Corrélation\", fontsize=12)\n",
    "    plt.xlabel(\"Date\", fontsize=12)\n",
    "    plt.title(\"Évolution des Régimes de Corrélation (K-Means Temporel)\", \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.yticks(range(1, n_clusters+1), [f'Régime {i+1}' for i in range(n_clusters)])\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Application du clustering temporel\n",
    "temporal_labels, kmeans_model, correlation_data = perform_temporal_kmeans_clustering(\n",
    "    rolling_correlations, n_clusters=3\n",
    ")\n",
    "\n",
    "# Visualisation\n",
    "visualize_temporal_clusters(correlation_dates, temporal_labels, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e47a97",
   "metadata": {},
   "source": [
    "# 🔍 8. DÉTECTION AUTOMATIQUE DES CHANGEMENTS DE RÉGIME\n",
    "\n",
    "Utilisation de l'analyse de variance et de la détection de pics pour identifier \n",
    "automatiquement les périodes de changement structurel dans les corrélations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e64506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_regime_changes(rolling_correlations, dates):\n",
    "    \"\"\"\n",
    "    Détecte automatiquement les changements de régime basés sur la variance des corrélations\n",
    "    \n",
    "    Args:\n",
    "        rolling_correlations: Liste des matrices de corrélation\n",
    "        dates: Dates correspondantes\n",
    "    \n",
    "    Returns:\n",
    "        Indices des changements de régime, variance temporelle\n",
    "    \"\"\"\n",
    "    print(\"🔍 Détection automatique des changements de régime...\")\n",
    "    \n",
    "    # Calcul de la variance moyenne des corrélations dans le temps\n",
    "    variance_timeline = []\n",
    "    \n",
    "    for correlation_matrix in rolling_correlations:\n",
    "        # Extraction de la partie supérieure de la matrice (sans diagonale)\n",
    "        upper_triangle = correlation_matrix.where(\n",
    "            np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "        variance = np.nanvar(upper_triangle.values.flatten())\n",
    "        variance_timeline.append(variance)\n",
    "    \n",
    "    variance_timeline = np.array(variance_timeline)\n",
    "    \n",
    "    # Détection des pics de variance (changements de régime potentiels)\n",
    "    threshold_height = np.mean(variance_timeline) + 1.5 * np.std(variance_timeline)\n",
    "    regime_change_indices, properties = find_peaks(\n",
    "        variance_timeline, \n",
    "        height=threshold_height,\n",
    "        distance=10  # Distance minimale entre pics\n",
    "    )\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Graphique principal\n",
    "    plt.plot(dates, variance_timeline, \n",
    "             label='Variance des Corrélations', \n",
    "             linewidth=2, color='blue', alpha=0.7)\n",
    "    \n",
    "    # Ligne de seuil\n",
    "    plt.axhline(y=threshold_height, color='red', linestyle='--', \n",
    "                label=f'Seuil de détection ({threshold_height:.4f})', alpha=0.7)\n",
    "    \n",
    "    # Points de changement de régime\n",
    "    if len(regime_change_indices) > 0:\n",
    "        plt.scatter(dates[regime_change_indices], \n",
    "                   variance_timeline[regime_change_indices],\n",
    "                   color='red', s=100, marker='v', \n",
    "                   label='Changements de Régime Détectés', \n",
    "                   zorder=5)\n",
    "        \n",
    "        # Annotations\n",
    "        for idx in regime_change_indices:\n",
    "            plt.annotate(f'{dates[idx].strftime(\"%Y-%m-%d\")}',\n",
    "                        (dates[idx], variance_timeline[idx]),\n",
    "                        xytext=(10, 10), textcoords='offset points',\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.7),\n",
    "                        fontsize=10)\n",
    "    \n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Variance des Corrélations', fontsize=12)\n",
    "    plt.title('Détection Automatique des Changements de Régime', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Résultats\n",
    "    print(f\"\\n📊 Résultats de la détection:\")\n",
    "    print(f\"  Changements de régime détectés: {len(regime_change_indices)}\")\n",
    "    print(f\"  Variance moyenne: {np.mean(variance_timeline):.4f}\")\n",
    "    print(f\"  Variance maximale: {np.max(variance_timeline):.4f}\")\n",
    "    \n",
    "    if len(regime_change_indices) > 0:\n",
    "        print(f\"\\n📅 Dates des changements détectés:\")\n",
    "        for idx in regime_change_indices:\n",
    "            date_str = dates[idx].strftime(\"%Y-%m-%d\")\n",
    "            variance_val = variance_timeline[idx]\n",
    "            print(f\"    {date_str} (variance: {variance_val:.4f})\")\n",
    "    else:\n",
    "        print(f\"  ℹ️ Aucun changement de régime significatif détecté\")\n",
    "    \n",
    "    return regime_change_indices, variance_timeline\n",
    "\n",
    "# Détection des changements de régime\n",
    "regime_changes, variance_data = detect_regime_changes(rolling_correlations, correlation_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f92d3",
   "metadata": {},
   "source": [
    "# 🌟 9. CLUSTERING SPECTRAL OPTIMISÉ\n",
    "\n",
    "Application du clustering spectral avec optimisation du nombre de clusters par analyse du score silhouette.\n",
    "Cette méthode est particulièrement adaptée aux données non-convexes et aux structures complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_spectral_clustering(correlation_matrix, max_clusters=6):\n",
    "    \"\"\"\n",
    "    Effectue un clustering spectral optimisé sur la matrice de corrélation\n",
    "    \n",
    "    Args:\n",
    "        correlation_matrix: Matrice de corrélation\n",
    "        max_clusters: Nombre maximum de clusters à tester\n",
    "    \n",
    "    Returns:\n",
    "        Labels des clusters, nombre optimal de clusters, scores silhouette\n",
    "    \"\"\"\n",
    "    print(\"🌟 Clustering spectral avec optimisation...\")\n",
    "    \n",
    "    # Conversion de la matrice de corrélation en matrice de similarité\n",
    "    similarity_matrix = (correlation_matrix + 1) / 2  # Normalisation entre 0 et 1\n",
    "    \n",
    "    # Test de différents nombres de clusters\n",
    "    n_clusters_range = range(2, min(max_clusters+1, len(correlation_matrix.columns)))\n",
    "    silhouette_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for n_clusters in n_clusters_range:\n",
    "        spectral = SpectralClustering(\n",
    "            n_clusters=n_clusters, \n",
    "            affinity='precomputed', \n",
    "            random_state=42\n",
    "        )\n",
    "        labels = spectral.fit_predict(similarity_matrix)\n",
    "        score = silhouette_score(similarity_matrix, labels, metric='precomputed')\n",
    "        silhouette_scores.append(score)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        print(f\"  {n_clusters} clusters → Score silhouette: {score:.3f}\")\n",
    "    \n",
    "    # Visualisation des scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(n_clusters_range, silhouette_scores, marker='o', linewidth=2, markersize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('Nombre de clusters', fontsize=12)\n",
    "    plt.ylabel('Score silhouette', fontsize=12)\n",
    "    plt.title('Optimisation du nombre de clusters (Spectral Clustering)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xticks(list(n_clusters_range))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sélection du meilleur nombre de clusters\n",
    "    best_idx = np.argmax(silhouette_scores)\n",
    "    optimal_n_clusters = list(n_clusters_range)[best_idx]\n",
    "    optimal_labels = all_labels[best_idx]\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    print(f\"\\n🎯 Nombre optimal de clusters: {optimal_n_clusters} (score: {silhouette_scores[best_idx]:.3f})\")\n",
    "    \n",
    "    # Organisation des résultats par groupe\n",
    "    spectral_groups = {}\n",
    "    for i, crypto in enumerate(correlation_matrix.columns):\n",
    "        cluster_id = optimal_labels[i]\n",
    "        if cluster_id not in spectral_groups:\n",
    "            spectral_groups[cluster_id] = []\n",
    "        spectral_groups[cluster_id].append(crypto)\n",
    "    \n",
    "    for cluster_id, cryptos in spectral_groups.items():\n",
    "        print(f\"  Groupe {cluster_id + 1}: {', '.join(cryptos)}\")\n",
    "    \n",
    "    return optimal_labels, optimal_n_clusters, silhouette_scores, spectral_groups\n",
    "\n",
    "# Application du clustering spectral optimisé\n",
    "spectral_labels, optimal_n_clusters, silhouette_scores, spectral_groups = perform_spectral_clustering(\n",
    "    latest_correlation, max_clusters=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1cfaf5",
   "metadata": {},
   "source": [
    "# 📋 10. RAPPORT FINAL ET CONCLUSION\n",
    "\n",
    "Synthèse des analyses effectuées et des résultats obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48656cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report():\n",
    "    \"\"\"\n",
    "    Génère un rapport final avec tous les résultats de l'analyse\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "    print(\"📋 RAPPORT FINAL - ANALYSE DE CORRÉLATION DES CRYPTOMONNAIES\")\n",
    "    print(\"=\"*65)\n",
    "    \n",
    "    # 1. Résumé des données\n",
    "    print(f\"\\n📊 DONNÉES ANALYSÉES:\")\n",
    "    print(f\"  • Cryptomonnaies: {len(crypto_symbols)}\")\n",
    "    print(f\"  • Période d'analyse: {correlation_dates[0].date()} à {correlation_dates[-1].date()}\")\n",
    "    print(f\"  • Nombre de jours: {len(data_close)}\")\n",
    "    print(f\"  • Matrices de corrélation calculées: {len(rolling_correlations)}\")\n",
    "    \n",
    "    # 2. Résultats du clustering hiérarchique\n",
    "    print(f\"\\n🌳 CLUSTERING HIÉRARCHIQUE:\")\n",
    "    for cluster_id, cryptos in cluster_groups.items():\n",
    "        print(f\"    Groupe {cluster_id}: {', '.join(cryptos)}\")\n",
    "    \n",
    "    # 3. Résultats du clustering spectral\n",
    "    print(f\"\\n🌟 CLUSTERING SPECTRAL OPTIMAL ({optimal_n_clusters} clusters):\")\n",
    "    for cluster_id, cryptos in spectral_groups.items():\n",
    "        print(f\"    Groupe {cluster_id + 1}: {', '.join(cryptos)}\")\n",
    "    \n",
    "    # 4. Changements de régime\n",
    "    print(f\"\\n🔍 DÉTECTION DE RÉGIMES:\")\n",
    "    print(f\"  • Changements de régime détectés: {len(regime_changes)}\")\n",
    "    if len(regime_changes) > 0:\n",
    "        for idx in regime_changes:\n",
    "            print(f\"    - {correlation_dates[idx].strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # 5. Statistiques temporelles\n",
    "    temporal_counts = np.bincount(temporal_labels)\n",
    "    print(f\"\\n⏰ RÉGIMES TEMPORELS (K-Means):\")\n",
    "    for i, count in enumerate(temporal_counts):\n",
    "        percentage = count / len(temporal_labels) * 100\n",
    "        print(f\"    Régime {i+1}: {count} jours ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 6. Métriques de qualité\n",
    "    correlation_mean = latest_correlation.mean().mean()\n",
    "    correlation_std = np.std([corr.mean().mean() for corr in rolling_correlations])\n",
    "    \n",
    "    print(f\"\\n📊 MÉTRIQUES DE QUALITÉ:\")\n",
    "    print(f\"  • Corrélation moyenne actuelle: {correlation_mean:.3f}\")\n",
    "    print(f\"  • Volatilité des corrélations: {correlation_std:.3f}\")\n",
    "    print(f\"  • Score silhouette spectral: {silhouette_scores[optimal_n_clusters-2]:.3f}\")\n",
    "    \n",
    "    # 7. Recommandations\n",
    "    print(f\"\\n💡 RECOMMANDATIONS:\")\n",
    "    print(f\"  • Diversification: Investir dans différents clusters pour réduire le risque\")\n",
    "    print(f\"  • Surveillance: Monitorer les changements de régime pour ajuster la stratégie\")\n",
    "    print(f\"  • Gestion des risques: Vigilance accrue en période de forte corrélation\")\n",
    "    \n",
    "    print(f\"\\n✅ ANALYSE TERMINÉE AVEC SUCCÈS\")\n",
    "    print(\"=\"*65)\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "def save_all_results():\n",
    "    \"\"\"\n",
    "    Sauvegarde tous les résultats dans des fichiers CSV\n",
    "    \"\"\"\n",
    "    print(\"\\n💾 Sauvegarde des résultats finaux...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Rendements logarithmiques\n",
    "        log_returns.to_csv('log_returns_final.csv')\n",
    "        print(\"  ✅ log_returns_final.csv\")\n",
    "        \n",
    "        # 2. Résultats du clustering\n",
    "        clustering_results = pd.DataFrame({\n",
    "            'cryptocurrency': latest_correlation.columns,\n",
    "            'hierarchical_cluster': hierarchical_clusters,\n",
    "            'spectral_cluster': spectral_labels + 1,  # +1 pour commencer à 1\n",
    "            'temporal_cluster_final': temporal_labels[-1] + 1\n",
    "        })\n",
    "        clustering_results.to_csv('clustering_results_final.csv', index=False)\n",
    "        print(\"  ✅ clustering_results_final.csv\")\n",
    "        \n",
    "        # 3. Détection de régimes\n",
    "        regime_data = pd.DataFrame({\n",
    "            'date': correlation_dates,\n",
    "            'variance': variance_data,\n",
    "            'temporal_cluster': temporal_labels + 1\n",
    "        })\n",
    "        regime_data.to_csv('regime_detection_results.csv', index=False)\n",
    "        print(\"  ✅ regime_detection_results.csv\")\n",
    "        \n",
    "        print(f\"\\n🎯 Tous les fichiers sauvegardés avec succès!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Erreur lors de la sauvegarde: {e}\")\n",
    "\n",
    "# Génération du rapport final\n",
    "generate_final_report()\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "save_all_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a644a",
   "metadata": {},
   "source": [
    "# 💡 CONCLUSION ET RECOMMANDATIONS\n",
    "\n",
    "## 🎯 Principaux Résultats\n",
    "\n",
    "L'analyse de corrélation dynamique des cryptomonnaies a révélé plusieurs insights importants :\n",
    "\n",
    "1. **Groupes de Cryptomonnaies** : Le clustering a identifié des groupes distincts avec des comportements corrélés\n",
    "2. **Évolution Temporelle** : Les corrélations ne sont pas statiques et évoluent significativement \n",
    "3. **Changements de Régime** : Des périodes distinctes avec des structures de corrélation différentes\n",
    "4. **Diversification** : Possibilités d'optimisation de portefeuille basées sur les groupes identifiés\n",
    "\n",
    "## 💼 Recommandations pour les Investisseurs\n",
    "\n",
    "- **Diversification** : Répartir les investissements entre différents clusters\n",
    "- **Surveillance** : Monitorer les changements de régime pour ajuster la stratégie\n",
    "- **Gestion des Risques** : Être vigilant pendant les périodes de forte corrélation\n",
    "- **Opportunités** : Exploiter les décorrélations temporaires pour des stratégies de trading\n",
    "\n",
    "## 🔮 Améliorations Futures\n",
    "\n",
    "- Intégration de données macroéconomiques\n",
    "- Modèles prédictifs pour anticiper les changements\n",
    "- Analyse en temps réel avec APIs\n",
    "- Extension à d'autres classes d'actifs\n",
    "\n",
    "---\n",
    "\n",
    "**Projet réalisé dans le cadre de l'analyse de données financières - Data Mining**\n",
    "\n",
    "*Date de finalisation : Mai 2025*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
